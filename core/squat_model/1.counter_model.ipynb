{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Drawing helpers\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up important landmarks and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Data Frame\n",
    "\n",
    "According to my research *the correct form* for a squat is analyzed through the position of:\n",
    "- Back\n",
    "- Hip\n",
    "- Legs\n",
    "\n",
    "Therefore, there will be *9 keypoints* which will be extract from mediapipe in order to train or detect a correct form of a squat:\n",
    "- \"NOSE\",\n",
    "- \"LEFT_SHOULDER\",\n",
    "- \"RIGHT_SHOULDER\",\n",
    "- \"LEFT_HIP\",\n",
    "- \"RIGHT_HIP\",\n",
    "- \"LEFT_KNEE\",\n",
    "- \"RIGHT_KNEE\",\n",
    "- \"LEFT_ANKLE\",\n",
    "- \"RIGHT_ANKLE\"\n",
    "\n",
    "The data frame will be saved in a .csv file.\n",
    "\n",
    "A data frame will contains a \"Label\" columns which represent the label of a data point.\n",
    "\n",
    "There are another 9 x 4 columns represent 9 features of a human pose that are important for a squat.\n",
    "In that each landmark's info will be flatten\n",
    "\n",
    "According to the [Mediapipe documentation](https://google.github.io/mediapipe/solutions/pose#python-solution-api),\n",
    "Each landmark consists of the following:\n",
    "- x and y: Landmark coordinates normalized to [0.0, 1.0] by the image width and height respectively.\n",
    "- z: Represents the landmark depth with the depth at the midpoint of hips being the origin, and the smaller the value the closer the landmark is to the camera. The magnitude of z uses roughly the same scale as x.\n",
    "- visibility: A value in [0.0, 1.0] indicating the likelihood of the landmark being visible (present and not occluded) in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# Determine important landmarks for squat\n",
    "IMPORTANT_LMS = [\n",
    "    \"NOSE\",\n",
    "    \"LEFT_SHOULDER\",\n",
    "    \"RIGHT_SHOULDER\",\n",
    "    \"LEFT_HIP\",\n",
    "    \"RIGHT_HIP\",\n",
    "    \"LEFT_KNEE\",\n",
    "    \"RIGHT_KNEE\",\n",
    "    \"LEFT_ANKLE\",\n",
    "    \"RIGHT_ANKLE\"\n",
    "]\n",
    "\n",
    "# Generate all columns of the data frame\n",
    "\n",
    "landmarks = [\"label\"] # Label column\n",
    "\n",
    "for lm in IMPORTANT_LMS:\n",
    "    landmarks += [f\"{lm.lower()}_x\", f\"{lm.lower()}_y\", f\"{lm.lower()}_z\", f\"{lm.lower()}_v\"]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "def rescale_frame(frame, percent=50):\n",
    "    '''\n",
    "    Rescale a frame to a certain percentage compare to its original frame\n",
    "    '''\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)\n",
    "    \n",
    "\n",
    "def init_csv(dataset_path: str):\n",
    "    '''\n",
    "    Create a blank csv file with just columns\n",
    "    '''\n",
    "\n",
    "    # Write all the columns to a file\n",
    "    with open(dataset_path, mode=\"w\", newline=\"\") as f:\n",
    "        csv_writer = csv.writer(f, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csv_writer.writerow(landmarks)\n",
    "\n",
    "        \n",
    "def export_landmark_to_csv(dataset_path: str, results, action: str) -> None:\n",
    "    '''\n",
    "    Export Labeled Data from detected landmark to csv\n",
    "    '''\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "    keypoints = []\n",
    "\n",
    "    try:\n",
    "        # Extract coordinate of important landmarks\n",
    "        for lm in IMPORTANT_LMS:\n",
    "            keypoint = landmarks[mp_pose.PoseLandmark[lm].value]\n",
    "            keypoints.append([keypoint.x, keypoint.y, keypoint.z, keypoint.visibility])\n",
    "        \n",
    "        keypoints = list(np.array(keypoints).flatten())\n",
    "\n",
    "        # Insert action as the label (first column)\n",
    "        keypoints.insert(0, action)\n",
    "\n",
    "        # Append new row to .csv file\n",
    "        with open(dataset_path, mode=\"a\", newline=\"\") as f:\n",
    "            csv_writer = csv.writer(f, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer.writerow(keypoints)\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "\n",
    "def concat_csv_files_with_same_headers(file_paths: list, saved_path: str):\n",
    "    '''\n",
    "    Concat different csv files\n",
    "    '''\n",
    "    all_df = []\n",
    "    for path in file_paths:\n",
    "        df = pd.read_csv(path, index_col=None, header=0)\n",
    "        all_df.append(df)\n",
    "    \n",
    "    results = pd.concat(all_df, axis=0, ignore_index=True)\n",
    "    results.to_csv(saved_path, sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "def describe_dataset(dataset_path: str):\n",
    "    ''''''\n",
    "\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    print(f\"Headers: {list(data.columns.values)}\")\n",
    "    print(f'Number of rows: {data.shape[0]} \\nNumber of columns: {data.shape[1]}\\n')\n",
    "    print(f\"Labels: \\n{data['label'].value_counts()}\\n\")\n",
    "    print(f\"Missing values: {data.isnull().values.any()}\\n\")\n",
    "    duplicate = data[data.duplicated()]\n",
    "    print(f\"Duplicate Rows : {duplicate.sum(axis=1)}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def round_up_metric_results(results) -> list:\n",
    "    '''Round up metrics results such as precision score, recall score, ...'''\n",
    "    return list(map(lambda el: round(el, 3), results))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract data for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "DATASET_PATH = \"./train.csv\"\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/squat/squat_posture.mp4\")\n",
    "up_save_count = 0\n",
    "down_save_count = 0\n",
    "\n",
    "# init_csv(DATASET_PATH)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Reduce size of a frame\n",
    "        image = rescale_frame(image, 60)\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks and connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=4), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Display the saved count\n",
    "        cv2.putText(image, f\"UP saved: {up_save_count}\", (50, 150), cv2.FONT_HERSHEY_COMPLEX, 2, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, f\"DOWN saved: {down_save_count}\", (50, 200), cv2.FONT_HERSHEY_COMPLEX, 2, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"CV2\", image)\n",
    "\n",
    "         # Pressed key for action\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if k == ord('d'): \n",
    "            export_landmark_to_csv(DATASET_PATH, results, \"down\")\n",
    "            down_save_count += 1\n",
    "        elif k == ord(\"u\"):\n",
    "            export_landmark_to_csv(DATASET_PATH, results, \"up\")\n",
    "            up_save_count += 1\n",
    "        # Press q to stop\n",
    "        elif k == ord(\"q\"):\n",
    "            break\n",
    "        else: continue\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # (Optional)Fix bugs cannot close windows in MacOS (https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv)\n",
    "    for i in range (1, 5):\n",
    "        cv2.waitKey(1)\n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "df = describe_dataset(DATASET_PATH)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract data for test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "TEST_DATASET_PATH = \"./test.csv\"\n",
    "\n",
    "cap = cv2.VideoCapture(\"../data/squat/squat_test_4.mp4\")\n",
    "up_save_count = 0\n",
    "down_save_count = 0\n",
    "\n",
    "# init_csv(TEST_DATASET_PATH)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Reduce size of a frame\n",
    "        image = rescale_frame(image, 60)\n",
    "        # image = cv2.flip(image, 1)\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Recolor image from BGR to RGB for mediapipe\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw landmarks and connections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS, mp_drawing.DrawingSpec(color=(244, 117, 66), thickness=2, circle_radius=4), mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Display the saved count\n",
    "        cv2.putText(image, f\"UP saved: {up_save_count}\", (50, 150), cv2.FONT_HERSHEY_COMPLEX, 2, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, f\"DOWN saved: {down_save_count}\", (50, 200), cv2.FONT_HERSHEY_COMPLEX, 2, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"CV2\", image)\n",
    "\n",
    "         # Pressed key for action\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if k == ord('d'): \n",
    "            export_landmark_to_csv(TEST_DATASET_PATH, results, \"down\")\n",
    "            down_save_count += 1\n",
    "        elif k == ord(\"u\"):\n",
    "            export_landmark_to_csv(TEST_DATASET_PATH, results, \"up\")\n",
    "            up_save_count += 1\n",
    "        # Press q to stop\n",
    "        elif k == ord(\"q\"):\n",
    "            break\n",
    "        else: continue\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # (Optional)Fix bugs cannot close windows in MacOS (https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv)\n",
    "    for i in range (1, 5):\n",
    "        cv2.waitKey(1)\n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train custom model using Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Read and describe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# Brief describe of the dataset\n",
    "df = describe_dataset(\"./train.csv\")\n",
    "sns.countplot(x='label', data=df, palette=\"Set1\") \n",
    "df.loc[df[\"label\"] == \"down\", \"label\"] = 0\n",
    "df.loc[df[\"label\"] == \"up\", \"label\"] = 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "source": [
    "# Extract features and class\n",
    "X = df.drop(\"label\", axis=1) # features\n",
    "y = df[\"label\"].astype(\"int\")\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = pd.DataFrame(sc.fit_transform(X))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "source": [
    "# Split train set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "y_test.head(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Train Machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "source": [
    "algorithms =[(\"LR\", LogisticRegression()),\n",
    "         (\"SVC\", SVC(probability=True)),\n",
    "         ('KNN',KNeighborsClassifier()),\n",
    "         (\"DTC\", DecisionTreeClassifier()),\n",
    "         (\"SGDC\", CalibratedClassifierCV(SGDClassifier())),\n",
    "         (\"NB\", GaussianNB()),\n",
    "         ('RF', RandomForestClassifier()),]\n",
    "\n",
    "models = {}\n",
    "final_results = []\n",
    "\n",
    "for name, model in algorithms:\n",
    "    trained_model = model.fit(X_train, y_train)\n",
    "    models[name] = trained_model\n",
    "\n",
    "    # Evaluate model\n",
    "    model_results = model.predict(X_test)\n",
    "\n",
    "    p_score = precision_score(y_test, model_results, average=None, labels=[0, 1])\n",
    "    a_score = accuracy_score(y_test, model_results)\n",
    "    r_score = recall_score(y_test, model_results, average=None, labels=[0, 1])\n",
    "    f1_score_result = f1_score(y_test, model_results, average=None, labels=[0, 1])\n",
    "    cm = confusion_matrix(y_test, model_results, labels=[0, 1])\n",
    "    final_results.append(( name,  round_up_metric_results(p_score), a_score, round_up_metric_results(r_score), round_up_metric_results(f1_score_result), cm))\n",
    "\n",
    "# Sort results by F1 score\n",
    "final_results.sort(key=lambda k: sum(k[4]), reverse=True)\n",
    "pd.DataFrame(final_results, columns=[\"Model\", \"Precision Score\", \"Accuracy score\", \"Recall Score\", \"F1 score\", \"Confusion Matrix\"])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "source": [
    "test_df = describe_dataset(\"./test.csv\")\n",
    "test_df.loc[test_df[\"label\"] == \"down\", \"label\"] = 0\n",
    "test_df.loc[test_df[\"label\"] == \"up\", \"label\"] = 1\n",
    "\n",
    "test_x = test_df.drop(\"label\", axis=1)\n",
    "test_y = test_df[\"label\"].astype(\"int\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "source": [
    "testset_final_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Evaluate model\n",
    "    model_results = model.predict(test_x)\n",
    "\n",
    "    p_score = precision_score(test_y, model_results, average=\"weighted\")\n",
    "    a_score = accuracy_score(test_y, model_results)\n",
    "    r_score = recall_score(test_y, model_results, average=\"weighted\")\n",
    "    f1_score_result = f1_score(test_y, model_results, average=\"weighted\")\n",
    "    cm = confusion_matrix(test_y, model_results, labels=[0, 1])\n",
    "    testset_final_results.append(( name,  (p_score), a_score, (r_score), (f1_score_result), cm ))\n",
    "\n",
    "\n",
    "testset_final_results.sort(key=lambda k: k[4], reverse=True)\n",
    "eval_df = pd.DataFrame(testset_final_results, columns=[\"Model\", \"Precision score\", \"Accuracy score\", \"Recall score\", \"F1 score\", \"Confusion Matrix\"])\n",
    "eval_df = eval_df.sort_values(by=['F1 score'], ascending=False).reset_index(drop=True)\n",
    "eval_df.to_csv(f\"evaluation.csv\", sep=',', encoding='utf-8', index=False)\n",
    "eval_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Dumped model using pickle\n",
    "\n",
    "According to the evaluations, there are multiple good models at the moment, therefore, I will pick the Random Forrest model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "source": [
    "with open(\"./model/sklearn_models.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models, f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "source": [
    "with open(\"./model/LR_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models[\"LR\"], f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "source": [
    "with open(\"./model/SGDC_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models[\"SGDC\"], f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "source": [
    "best_model = models[\"LR\"]\n",
    "y_predictions = best_model.predict(test_x)\n",
    "\n",
    "p_score = precision_score(test_y, y_predictions, labels=[0, 1], average=None)\n",
    "r_score = recall_score(test_y, y_predictions, labels=[0, 1], average=None)\n",
    "f1_score_result = f1_score(test_y, y_predictions, labels=[0, 1], average=None)\n",
    "\n",
    "p_score, r_score, f1_score_result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "source": [
    "KNN_cm = eval_df[ eval_df[\"Model\"] == 'LR' ][\"Confusion Matrix\"].values[0]\n",
    "\n",
    "cm_array_df = pd.DataFrame(KNN_cm, index=[\"down\", \"up\"], columns=[\"down\", \"up\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6)) \n",
    "sns.heatmap(cm_array_df, linewidths=1, annot=True, ax=ax, fmt='g', cmap=\"crest\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "source": [
    "def to_labels(y_pred, y_pred_proba, threshold):\n",
    "    '''Return prediction taking confidence threshold into account'''\n",
    "    results = []\n",
    "\n",
    "    for index, predicted_class in enumerate(y_pred):\n",
    "        prediction_probabilities = y_pred_proba[index]\n",
    "        class_prediction_probability = round(prediction_probabilities[np.argmax(prediction_probabilities)], 2)\n",
    "\n",
    "        results.append(predicted_class if class_prediction_probability >= threshold else -1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_correlation_score_confidence(test_x, test_y):\n",
    "    '''Calculate correlation between Precision score/Recall score/F1 score and confidence threshold'''\n",
    "    y_predictions = best_model.predict(test_x)\n",
    "    y_predict_proba = best_model.predict_proba(test_x)\n",
    "\n",
    "    thresholds = list(np.arange(0, 1.05, 0.01))\n",
    "\n",
    "    f1_score_results = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        true_predictions = to_labels(y_predictions, y_predict_proba, threshold)\n",
    "        f1_s = list(f1_score(test_y, true_predictions, labels=[0, 1], average=None))\n",
    "        all_class_f1 = f1_score(test_y, true_predictions, labels=[0, 1], average=\"weighted\")\n",
    "        f1_s.append(all_class_f1)\n",
    "        f1_score_results.append(f1_s)\n",
    "    \n",
    "    return thresholds, f1_score_results\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "source": [
    "thresholds, f1_scores = calculate_correlation_score_confidence(test_x, test_y)\n",
    "\n",
    "first_class = [ el[0] for el in f1_scores ]\n",
    "second_class = [ el[1] for el in f1_scores ]\n",
    "all_classes = [ el[2] for el in f1_scores ]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(thresholds, first_class, label = \"F1 Score - Correct class\")\n",
    "plt.plot(thresholds, second_class, label = \"F1 Score - Incorrect class\")\n",
    "plt.plot(thresholds, all_classes, label = \"F1 Score - All classes\", linewidth=2.0, color=\"blue\")\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0.025, 1])\n",
    "plt.xlabel(\"Thresholds\", fontsize = 12)\n",
    "plt.ylabel(\"F1 Score\", fontsize = 12)\n",
    "# plt.axvline(thresholds[np.argmin(abs(precision-recall))], color=\"k\", ls = \"--\")\n",
    "# plt.title(label = F\"Threshold = {thresholds[np.argmin(abs(precision-recall))]:.3f}\", fontsize = 12)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "source": [
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = best_model.predict_proba(test_x)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(test_y, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = threshold[optimal_idx]\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "\n",
    "# method I: plt\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--', label=\"Random Guess\")\n",
    "plt.legend(loc=4)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9260f401923fb5c4108c543a7d176de9733d378b3752e49535ad7c43c2271b65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
